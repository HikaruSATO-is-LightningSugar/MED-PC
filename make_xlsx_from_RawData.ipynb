{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生のデータファイルの置き場所\n",
    "# dockerの場合\n",
    "path_to_RawData_folder = '/home/biofunc/data/ラット行動実験データ（生体機能・組織培養室PC）/Data'\n",
    "\n",
    "dict_of_target = {\n",
    "   'DIFF':{\n",
    "        'subjects':['SH010', 'SH012', 'SH014', 'SH015', 'SH017', 'SH022', 'SH024'],\n",
    "        'MSNs':['3b_disc-Diff', 'AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Diff_rft'],\n",
    "        'Index':['X', 'Y']\n",
    "    },\n",
    "    'MIXED':{\n",
    "        'subjects':['I321', 'I322', 'I325', 'SH011', 'SH013', 'SH020'],\n",
    "        'MSNs':['3a_disc-Mixed', 'AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Mixed_rft'],\n",
    "        'Index':['X', 'Y']\n",
    "    }\n",
    "}\n",
    "\n",
    "window_size = [10, 20, 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ↑　プログラムを動かすたびに編集するところ　↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: japanize-matplotlib in /opt/conda/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from japanize-matplotlib) (3.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from matplotlib->japanize-matplotlib) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#とりあえず、json型で出力することにした\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#例えば、seaborn を利用している場合であれば sns.set() などで描画フォントが seaborn のデフォルトに上書きされ、日本語表示がされなくなります。\n",
    "#sns.set(font=\"IPAexGothic\") のように利用フォントに IPAexGothic を設定するか、フォント上書き後に japanize_matplotlib.japanize() を利用するなどで日本語表示できるはずです。\n",
    "import japanize_matplotlib\n",
    "japanize_matplotlib.japanize()\n",
    "\n",
    "#############################################################################\n",
    "# 参考：jsonの構造\n",
    "# ラット一匹につき、jsonファイルは一つ\n",
    "{ \n",
    "    'rat_name':{\n",
    "\n",
    "        'session_1':{\n",
    "            \"Start Date\": \"10/04/22\",\n",
    "            \"End Date\": \"10/04/22\",\n",
    "            \"Subject\": \"sh010\",\n",
    "            \"MSN\": \"1b_mag-pel\",\n",
    "            \"A\": \"3.000\",\n",
    "            \"E\": [\n",
    "                [\"0\", \"3.000\", \"3.000\", \"2.000\", \"2.000\", \"4.000\"],\n",
    "                [\"5\", \"1.000\", \"1.000\", \"2.000\", \"1.000\", \"0.000\"],\n",
    "            ]\n",
    "        },\n",
    "\n",
    "        'session_2':{\n",
    "            \"Start Date\": \"10/05/22\",\n",
    "            \"End Date\": \"10/05/22\",\n",
    "            \"Subject\": \"sh010\",\n",
    "            \"MSN\": \"1b_mag-pel\",\n",
    "            \"A\": \"3.000\",\n",
    "            \"E\": [\n",
    "                [\"0\", \"3.000\", \"3.000\", \"2.000\", \"2.000\", \"4.000\"],\n",
    "                [\"5\", \"1.000\", \"1.000\", \"2.000\", \"1.000\", \"0.000\"],\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "#############################################################################\n",
    "\n",
    "def make_output_folder(path_to_RawData_folder):\n",
    "    # 出力フォルダの作成\n",
    "    # 出力フォルダのパスを作る\n",
    "    output_timestamp = str(dt.now().strftime('%Y%m%d_%Hh%Mm%Ss'))\n",
    "    path_to_output_folder = f'{Path(path_to_RawData_folder).parent}/Output/{output_timestamp}_ラット行動実験データ'\n",
    "    if not os.path.exists(path_to_output_folder):\n",
    "        os.makedirs(path_to_output_folder)\n",
    "    return path_to_output_folder\n",
    "\n",
    "\n",
    "# 生のデータファイルを読み込み、json型に変換する関数\n",
    "def make_json_from_RawData(path_to_a_RawData_folder, path_to_output_folder):\n",
    "    #ファイルを読み込み\n",
    "    path_of_all_raw_data = sorted(glob.glob(path_to_a_RawData_folder + '/*'))\n",
    "    # さらに、サブディレクトリを除去\n",
    "    path_of_all_raw_data = [a_path for a_path in path_of_all_raw_data if not os.path.isdir(a_path)]\n",
    "\n",
    "    #データ項目の正規表現パターン\n",
    "    pattern_alphabet_index = '(^[a-zA-Z]+[a-zA-Z|\\s]*)\\W.*'  #先頭がアルファベットの場合、最初の「:」までの文字列\n",
    "    pattern_number_index = '^[\\s]+([0-9]+)\\W.*'        #先頭が数字の場合、最初の「:」までの文字列\n",
    "    pattern_body = '^[\\w|\\s]+\\W+(.*)[\\r\\n|\\n|\\r]'   #最初の「:」から、改行までの文字列\n",
    "\n",
    "    #jsonの出力ファイルの保存先を作る\n",
    "    path_to_json_data_folder = f'{path_to_output_folder}/json_from_RawData'\n",
    "    if os.path.exists(path_to_json_data_folder):\n",
    "        shutil.rmtree(path_to_json_data_folder)\n",
    "        os.mkdir(path_to_json_data_folder)\n",
    "    elif  not os.path.exists(path_to_json_data_folder):\n",
    "        os.mkdir(path_to_json_data_folder)\n",
    "    \n",
    "    # ラット一匹分のデータを取り込み、json型に変換する\n",
    "    for path_of_a_raw_data in path_of_all_raw_data:\n",
    "        # 一匹分のデータを格納するリスト\n",
    "        output_of_a_raw_data = []\n",
    "        # ファイル名からラット名を取得\n",
    "        name_of_the_rat = os.path.basename(path_of_a_raw_data).split('.')[0].upper()\n",
    "        # ファイルを開く\n",
    "        \n",
    "        with open(path_of_a_raw_data, 'r') as a_raw_data:\n",
    "            # ファイルを一行ずつ読み込み\n",
    "            list_a_raw_data = a_raw_data.readlines()\n",
    "            # ファイルの先頭にある「Start Date:」の行番号を取得\n",
    "            index_for_crop = [i for i, a_row in enumerate(list_a_raw_data) if a_row.startswith('Start Date:')]\n",
    "            index_for_crop = index_for_crop + [len(list_a_raw_data) + 1]\n",
    "            # ファイルの先頭にある「Start Date:」の行番号を基準に、\n",
    "            # ファイルをセッションごとに分割\n",
    "            list_a_raw_data = [list_a_raw_data[index_for_crop[i]:index_for_crop[i+1]] for i in range(len(index_for_crop)-1)]\n",
    "\n",
    "            # 一セッション分のデータを取り込み、json型に変換する\n",
    "            for a_session_data in list_a_raw_data:\n",
    "                # 一セッション分のデータを格納するリスト\n",
    "                list_a_session_data = []\n",
    "                # 一セッション分のデータを一行ずつ読み込み\n",
    "                for a_row in a_session_data:\n",
    "                    # 一行分のデータを格納するリスト\n",
    "                    list_a_row_data = []\n",
    "                    # 一行分のデータを正規表現で分割\n",
    "                    alphabet_index = re.findall(pattern_alphabet_index, a_row)\n",
    "                    number_index = re.findall(pattern_number_index, a_row)\n",
    "                    a_body = re.findall(pattern_body, a_row)\n",
    "\n",
    "                    # 以下の場合分けの概要\n",
    "                    # 1) 行名がアルファベットの場合\n",
    "                        # a) 値の場合       →　そのまま代入\n",
    "                        # b) 値なしの場合   →　次の行から行列がはじまる\n",
    "                    # 2) 行名が数字の場合\n",
    "                        # a) 行列     →　行列を統合する\n",
    "\n",
    "                    # 行名がアルファベットの場合\n",
    "                    if alphabet_index != []:\n",
    "                        list_a_row_data += alphabet_index\n",
    "                        # 値が存在するときは、\n",
    "                        # 値は行列ではありえないので、素直に挿入\n",
    "                        if a_body !=['']:\n",
    "                            list_a_row_data += a_body\n",
    "                        # 値が存在しないときは、\n",
    "                        # 次の行から行列がはじまるので、空のリストを挿入\n",
    "                        elif a_body ==['']:\n",
    "                            list_a_row_data += [[]]\n",
    "                        list_a_session_data += [list_a_row_data]\n",
    "                    # 行名が数字の場合\n",
    "                    elif number_index != []:\n",
    "                        # 行列の場合は、行列を統合する\n",
    "                        a_new_row = number_index + a_body[0].split()\n",
    "                        # 「list_a_session_data」の一番最後のリストに、どんどんブッ込んで行く\n",
    "                        # そして、次の行名(アルファベット)が間にはさまるので、別の行列同士がくっつく事はない\n",
    "                        list_a_session_data[-1][1] += [a_new_row]\n",
    "                    # 場合分け終了 \n",
    "\n",
    "                # 一セッション分のデータ処理が終わったので、個体のデータに統合\n",
    "                output_of_a_raw_data += [list_a_session_data]\n",
    "            # 一匹分のデータ処理が終わり\n",
    "            # [[key, value], [key, value], [], ....]　から　{key:value, key:value, :, ......}　に変換\n",
    "            output_of_a_raw_data = [dict(i) for i in output_of_a_raw_data]\n",
    "\n",
    "            # セッションの開始日時を取得する関数\n",
    "            def get_date(a: dict):\n",
    "                #同じ日付のファイルがる場合、上書きされてしまうので、時間もふくめてファイル名をつける\n",
    "                start_time = dt.strptime(a[\"Start Date\"] + a[\"Start Time\"], '%m/%d/%y%H:%M:%S') #.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "                return(start_time)\n",
    "            \n",
    "            # [{session_1}, {session_2}, {}, {}, ....] から　{subject: {title_1:session_1, title_2:session_2, :, :, ....}}　に変換\n",
    "            output_of_a_raw_data = [[f'{get_date(a_session)}_{a_session[\"MSN\"]}', a_session] for a_session in output_of_a_raw_data]\n",
    "            output_of_a_raw_data = {name_of_the_rat: dict(output_of_a_raw_data)}\n",
    "            # json型に変換\n",
    "            with open(f'{path_to_json_data_folder}/{name_of_the_rat}.json' ,'w') as output_file:\n",
    "                json.dump(output_of_a_raw_data, output_file, indent=4, ensure_ascii=False)  # これを、そのままjson型として保存する\n",
    "            print(f'{name_of_the_rat}: \\tis converted to json format.')\n",
    "\n",
    "# (新)MED-PC操作ミス等のエラーを修復する関数\n",
    "def repaire_known_issue_of_json(path_to_output_folder):\n",
    "    path_to_json_folder = f'{path_to_output_folder}/json_from_RawData'\n",
    "\n",
    "    ##############################################################################################\n",
    "    # I321、R322、R323、R324、R325のMSNによる列数の違いを修復する。\n",
    "\n",
    "    #jsonファイルの中身を引越しする前に、この処理をするということにする。\n",
    "    # 修復の処理を順不同にするには、、？\n",
    "    # いや、引越しの場合データの置き場所が変わるので順不同にはできない。\n",
    "\n",
    "    before_edited_json_subject_session_dict = {\n",
    "        'I321': 'AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Mixed_rft',\n",
    "        'R322': 'AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Mixed_rft',\n",
    "        'R323': 'AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Diff_rft',\n",
    "        'R324': 'AuX_Disc2(pretrain_sgl_lev_Mixed_rft)[self-started]',\n",
    "        'R325': 'AuX_Disc2(pretrain_sgl_lev_Mixed_rft)[self-started]'\n",
    "    }\n",
    "\n",
    "    for a_subject_key in     before_edited_json_subject_session_dict.keys():\n",
    "        for a_session_key in     before_edited_json_subject_session_dict[a_subject_key]:\n",
    "            # 修復するjsonファイルが存在するかどうか。\n",
    "            if not os.path.exists(f'{path_to_json_folder}/{a_subject_key}.json'):\n",
    "                pass\n",
    "            # 修復するjsonファイルが存在する場合\n",
    "            elif os.path.exists(f'{path_to_json_folder}/{a_subject_key}.json'):\n",
    "                with open(f'{path_to_json_folder}/{a_subject_key}.json', 'r') as f:\n",
    "                    a_json = json.load(f)\n",
    "                    new_json = {}\n",
    "\n",
    "                    # 一匹分のデータ処理(普通は一匹だけしか保存されてないはず)\n",
    "                    for a_subject_key in a_json.keys():\n",
    "                        # 一セッション分のデータ処理\n",
    "                        for a_session_key in a_json[a_subject_key].keys():\n",
    "                            if a_json[a_subject_key][a_session_key]['MSN'] == before_edited_json_subject_session_dict[a_subject_key]:\n",
    "                                df_X = pd.DataFrame(a_json[a_subject_key][a_session_key]['X']).astype('float64')\n",
    "                                df_Y = pd.DataFrame(a_json[a_subject_key][a_session_key]['Y']).astype('float64')\n",
    "\n",
    "                                # 0で埋まっているデーブルの場合、刺激=0、レバーチョイス＝0ということになってしまい、\n",
    "                                # 「正解」と判定されてしまう。\n",
    "                                index_of_last_row = 0\n",
    "                                for (index_df_X, row_df_X), (index_df_Y, row_df_Y) in zip(df_X.iterrows(), df_Y.iterrows()):\n",
    "                                    if row_df_X[1:].sum() + row_df_Y[1:].sum() != 0:\n",
    "                                        index_of_last_row = index_df_X\n",
    "                                    elif row_df_X[1:].sum() + row_df_Y[1:].sum() == 0:\n",
    "                                        pass\n",
    "                                df_X = df_X.iloc[:index_of_last_row+1, :]\n",
    "                                df_Y = df_Y.iloc[:index_of_last_row+1, :]\n",
    "                                #df_Xとdf_Yの列数は、「行番号の列」と「データの列」の合計\n",
    "                                # データの列が５列の場合は、合計６列なので、\n",
    "                                # 欠損しているデータ列として、「報酬の種類」の列を追加する\n",
    "                                if len(df_X.columns) == 6:\n",
    "                                    # X6の列の場合は、emptyなので0を入れるだけ。ソートも必要ない\n",
    "                                    df_X.insert(6, 6, 0.0)\n",
    "                                    if len(df_Y.columns) == 6:\n",
    "                                        # 3はないことに注意！\n",
    "                                        df_Y.columns = [0, 1, 2, 4, 5, 6]\n",
    "                                        #正解はとりあえずペレット(=0)一択として代入した\n",
    "                                        df_Y.loc[df_Y[2] <= 2, 3] = (df_Y[1]==df_Y[2])*0 | (df_Y[1]!=df_Y[2])+1\n",
    "                                        ####################\n",
    "                                        #print('check point 1')\n",
    "                                        #display(df_Y)\n",
    "                                        ####################\n",
    "                                        # ソート\n",
    "                                        df_Y.sort_index(axis=1, inplace=True)\n",
    "                                        # jsonに戻す。valueだけだとndarrayになって以下のエラーが出るので、tolist()でlistに変換する\n",
    "                                        # TypeError: Object of type ndarray is not JSON serializable\n",
    "                                        a_json[a_subject_key][a_session_key]['X'] = df_X.values.tolist()\n",
    "                                        a_json[a_subject_key][a_session_key]['Y'] = df_Y.values.tolist()\n",
    "                                        # 修復したものを新しいjsonに保存\n",
    "                                        new_json.update({a_session_key:a_json[a_subject_key][a_session_key]})\n",
    "                                # XとYがそれぞれ７列のときは、何もしない\n",
    "                                elif len(df_X.columns) == 7:\n",
    "                                    if len(df_Y.columns) == 7:\n",
    "                                        new_json.update({a_session_key:a_json[a_subject_key][a_session_key]}) \n",
    "                                #XとYがそれぞれ違う列数のときは、エラーを出す\n",
    "                                else:\n",
    "                                    print(f'the number of X and Y columns are different. X:{len(df_X.columns)}, Y:{len(df_Y.columns)}')\n",
    "                                    print(f'{a_subject_key}, {a_session_key}')\n",
    "                                    print(f'{a_json[a_subject_key][a_session_key][\"MSN\"]}')\n",
    "                            else:\n",
    "                                new_json.update({a_session_key:a_json[a_subject_key][a_session_key]})\n",
    "                    # 一匹分のデータ処理が終わったら、新しいjsonファイルを保存\n",
    "                    # ほんとはファイル名にrepairedをつけたいが、次の処理で処理でpath名を指定するために、つけないでおく\n",
    "                    # 正規表現で指定できるなら、つけてもいいかも\n",
    "                    with open(f'{path_to_json_folder}/{a_subject_key}.(6th_Columns_Added).json', 'w') as f:\n",
    "                        json.dump({a_subject_key:new_json}, f, indent=4, ensure_ascii=False)\n",
    "                        os.remove(f'{path_to_json_folder}/{a_subject_key}.json')\n",
    "                        print(f'{a_subject_key}: \\thas been added 6th column to X and Y')\n",
    "    # I321終わり\n",
    "    ##############################################################################################\n",
    "\n",
    "    ##############################################################################################\n",
    "    #保存先を間違ったセッションデータの引越し・新規保存、間違いjsonの削除\n",
    "    Tsubject_Fsubject_THEsession_dict = {\n",
    "        'I322':{'R322':'2022-09-07 11:50:12_AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Mixed_rft'},\n",
    "        'I323':{'R323':'2022-09-07 12:54:45_AuX_Disc4(train)_Hi-Lef_Lo-Rig_w_correction_Diff_rft'},\n",
    "        'I324':{'R324':'2022-09-07 14:04:32_AuX_Disc2(pretrain_sgl_lev_Mixed_rft)[self-started]'},\n",
    "        'I325':{'R325':'2022-09-07 14:50:42_AuX_Disc2(pretrain_sgl_lev_Mixed_rft)[self-started]'},\n",
    "        'SH011':{'SH010':'2022-10-22 15:43:27_3a_disc-Mixed'},\n",
    "        'SH017':{'SH016':'2022-11-29 16:16:20_3b_disc-Diff'}\n",
    "    }\n",
    "\n",
    "    for a_Tsubject in Tsubject_Fsubject_THEsession_dict.keys():\n",
    "        # 単純にdict.key()で値を取り出すと、dict_keys型になってしまうので、list()でlist型に変換する\n",
    "        a_Fsubject = list(Tsubject_Fsubject_THEsession_dict[a_Tsubject].keys())[0]\n",
    "        missed_session = Tsubject_Fsubject_THEsession_dict[a_Tsubject][a_Fsubject]\n",
    "\n",
    "        # 目的のjsonファイルのパス\n",
    "\n",
    "        #path_to_True_json = f'{path_to_json_folder}/{a_Tsubject}.json'\n",
    "        #path_to_False_json = f'{path_to_json_folder}/{a_Fsubject}.json'\n",
    "        path_to_True_json = glob.glob(f'{path_to_json_folder}/{a_Tsubject}' + '.*')\n",
    "        path_to_False_json = glob.glob(f'{path_to_json_folder}/{a_Fsubject}' + '.*')\n",
    "        \n",
    "        # 目的のjsonファイルが両方存在しない場合は、エラーを出す\n",
    "        if (len(path_to_True_json) > 1) or (len(path_to_False_json) > 1):\n",
    "            print(f'{a_Tsubject}と{a_Fsubject}のjsonファイルが複数あります。')\n",
    "            break\n",
    "        elif (len(path_to_True_json) == 0) or (len(path_to_False_json) == 0):\n",
    "            print(f'{a_Tsubject}と{a_Fsubject}のjsonファイルがありません。')\n",
    "            break\n",
    "        else:\n",
    "            path_to_True_json = path_to_True_json[0]\n",
    "            path_to_False_json = path_to_False_json[0]\n",
    "\n",
    "        # 目的のjsonファイルが両方存在する場合は、jsonを読み込む\n",
    "        if os.path.exists(path_to_True_json) and os.path.exists(path_to_False_json):\n",
    "            with open(path_to_True_json, 'r') as f:\n",
    "                json_True = json.load(f)\n",
    "            with open(path_to_False_json, 'r') as f:\n",
    "                json_False = json.load(f)\n",
    "\n",
    "            for a_subject_of_json_False in json_False.keys():\n",
    "                if a_subject_of_json_False.upper() == a_Fsubject.upper():\n",
    "                    json_of_missed_session = json_False[a_Fsubject][missed_session]\n",
    "                    for a_subject_of_json_True in json_True.keys():\n",
    "                        if a_subject_of_json_True.upper() == a_Tsubject.upper():\n",
    "\n",
    "                            json_True[a_Tsubject].update({missed_session:json_of_missed_session})\n",
    "                            list_of_sorted_session_keys_of_Tsubject = sorted(list(json_True[a_Tsubject].keys()))\n",
    "                            json_True_repaired = {a_Tsubject:{i:json_True[a_Tsubject][i] for i in list_of_sorted_session_keys_of_Tsubject}}\n",
    "\n",
    "                            json_False_cleaned = {a_Fsubject:{i:json_False[a_Fsubject][i] for i in json_False[a_Fsubject].keys() if i != missed_session}}\n",
    "\n",
    "                    with open(f'{path_to_json_folder}/{a_Tsubject}.(repaired).json', 'w') as f:\n",
    "                        json.dump(json_True_repaired, f, indent=4, ensure_ascii=False)\n",
    "                        os.remove(path_to_True_json)\n",
    "                    with open(f'{path_to_json_folder}/{a_Fsubject}.(cleaned).json', 'w') as f:\n",
    "                        json.dump(json_False_cleaned, f, indent=4, ensure_ascii=False)\n",
    "                        os.remove(path_to_False_json)\n",
    "                    print(f'{a_Tsubject} \\t& {a_Fsubject} \\tare repaired !!')\n",
    "\n",
    "        \n",
    "    #保存先ミスのデータ引越し終わり\n",
    "    ##############################################################################################\n",
    "\n",
    "# (新)jsonファイルをxlsxファイルに変換する関数\n",
    "def make_xlsx_about_DOE_from_json(path_to_output_folder, dict_of_target, window_size):\n",
    "    # jsonファイルのパスを取得\n",
    "    path_to_json_folder = f'{path_to_output_folder}/json_from_RawData'\n",
    "    list_of_path_to_all_json = glob.glob(f'{path_to_json_folder}/*.json')\n",
    "\n",
    "    # 保存先のフォルダを作成(with Null)\n",
    "    path_to_xlsx_folder = f'{path_to_output_folder}/xlsx_from_json.(window{window_size}_withNULL)'\n",
    "    if os.path.exists(path_to_xlsx_folder):\n",
    "        shutil.rmtree(path_to_xlsx_folder)\n",
    "        os.mkdir(path_to_xlsx_folder)\n",
    "    elif not os.path.exists(path_to_xlsx_folder):\n",
    "        os.mkdir(path_to_xlsx_folder)\n",
    "    \n",
    "    # 保存先のフォルダを作成(without Null)\n",
    "    path_to_xlsx_folder_nosepoke_isnot_false = f'{path_to_output_folder}/xlsx_from_json.(window{window_size}_withoutNULL)'\n",
    "    if os.path.exists(path_to_xlsx_folder_nosepoke_isnot_false):\n",
    "        shutil.rmtree(path_to_xlsx_folder_nosepoke_isnot_false)\n",
    "        os.mkdir(path_to_xlsx_folder_nosepoke_isnot_false)\n",
    "    elif not os.path.exists(path_to_xlsx_folder_nosepoke_isnot_false):\n",
    "        os.mkdir(path_to_xlsx_folder_nosepoke_isnot_false)\n",
    "    \n",
    "    #このフォルダに関する処理を開始\n",
    "    df_of_this_folder = pd.DataFrame()\n",
    "    df_of_this_folder_nosepoke_isnot_false = pd.DataFrame()\n",
    "    # このMSNの種類ごとの処理を開始\n",
    "    for a_type_of_experiment in dict_of_target.keys():\n",
    "        ############################################################\n",
    "        display(a_type_of_experiment)\n",
    "        ############################################################\n",
    "        #この実験の対象となるsubjectの名前を取得\n",
    "        list_of_the_subjects = [i.upper() for i in  dict_of_target[a_type_of_experiment]['subjects']]\n",
    "        #この実験の対象となるMSNの名前を取得\n",
    "        list_of_the_MSNs = dict_of_target[a_type_of_experiment]['MSNs'] \n",
    "\n",
    "        \n",
    "        #json単位の処理を開始\n",
    "        df_of_this_type_of_experiment = pd.DataFrame()\n",
    "        df_of_this_type_of_experiment_nosepoke_isnot_false = pd.DataFrame()\n",
    "        # この実験の対象となるjsonファイルのパスを取捨選択\n",
    "        list_of_path_to_target_json = []\n",
    "        for a_path in list_of_path_to_all_json:\n",
    "            if os.path.basename(a_path).split('.')[0].upper() in list_of_the_subjects:\n",
    "                list_of_path_to_target_json.append(a_path)\n",
    "            else:\n",
    "                pass\n",
    "        # 対象となるjsonを一つずつ読み込んで、処理していく\n",
    "        for a_path in list_of_path_to_target_json:\n",
    "            with open(a_path, 'r') as f:\n",
    "                a_json = json.load(f)\n",
    "\n",
    "                # このjsonファイルに保存されているsubjectの名前を取得。（通常は1つだけ）\n",
    "                for a_subject_key in a_json.keys():\n",
    "                    ############################################################\n",
    "                    #display(f'check point {a_subject_key}')\n",
    "                    ############################################################\n",
    "                    df_of_this_subject = pd.DataFrame()\n",
    "                    df_of_this_subject_nosepoke_isnot_false = pd.DataFrame()\n",
    "                    \n",
    "                    first_datetime_of_this_MSN_session = 0\n",
    "                    is_this_first_session_of_a_MSN = True\n",
    "                    list_of_df_to_add_columns = []\n",
    "\n",
    "                    # このsubjectのsessionを取得。（通常はいっぱい）\n",
    "                    for a_session_key in a_json[a_subject_key].keys():\n",
    "                        ############################################################\n",
    "                        #display(f'check point {a_session_key}')\n",
    "                        ############################################################\n",
    "                        MSN_of_this_session = a_json[a_subject_key][a_session_key]['MSN']\n",
    "                        # このsessionのMSNが、対象となるMSNであるかを判定\n",
    "                        if MSN_of_this_session in list_of_the_MSNs:\n",
    "                            df_of_this_session = pd.DataFrame()\n",
    "                            df_of_this_session_nosepoke_isnot_false = pd.DataFrame()\n",
    "                            # このsessionのjsonを取得\n",
    "                            json_of_this_session = a_json[a_subject_key][a_session_key]\n",
    "                            # このsessionの開始時刻と終了時刻を取得\n",
    "                            start_datetime_of_this_session = dt.strptime(json_of_this_session['Start Date'] + json_of_this_session['Start Time'], '%m/%d/%y%H:%M:%S')\n",
    "                            end_datetime_of_this_session = dt.strptime(json_of_this_session['End Date'] + json_of_this_session['End Time'], '%m/%d/%y%H:%M:%S')\n",
    "                            # あるMSNでのsessionの内、これが一番最初のsessionかどうかを判定し、開始時刻を記録\n",
    "                            if is_this_first_session_of_a_MSN == True:\n",
    "                                first_datetime_of_this_MSN_session = start_datetime_of_this_session\n",
    "                                is_this_first_session_of_a_MSN = False\n",
    "                        \n",
    "############################################################\n",
    "# DOEの弁別訓練のための処理\n",
    "                            df_X = pd.DataFrame(json_of_this_session['X']).astype(float)\n",
    "                            df_Y = pd.DataFrame(json_of_this_session['Y']).astype(float)\n",
    "\n",
    "                            index_of_last_row = 0\n",
    "                            for (index_df_X, row_df_X), (index_df_Y, row_df_Y) in zip(df_X.iterrows(), df_Y.iterrows()):\n",
    "                                if row_df_X[1:].sum() + row_df_Y[1:].sum() != 0:\n",
    "                                    index_of_last_row = index_df_X\n",
    "                                elif row_df_X[1:].sum() + row_df_Y[1:].sum() == 0:\n",
    "                                    pass\n",
    "                            df_X = df_X.iloc[:index_of_last_row+1, :]\n",
    "                            df_Y = df_Y.iloc[:index_of_last_row+1, :]\n",
    "                            df_X[4] = df_X[4] * 0.01\n",
    "\n",
    "                            '''\n",
    "                            Data matrix of behavior (X, Y)\n",
    "                            1: Order of trial (order of nose-poke illuminations)\n",
    "                            2: Nose-poking in 20 min (0 = yes; 1 = no --> session termination)\n",
    "                            3: Nose-poking latency\n",
    "                            4: Nose-poking duration required\n",
    "                            5: Nose-poking duration (measured)\n",
    "                            6: empty\n",
    "\n",
    "                            7: Sd presented\n",
    "                            8: Lever pressed (0 = left, 1 = right, 2 = no-choice, 999 = immature trial)\n",
    "                            9: Outcome (0 = pellet, 1 = liquid, 2 = incorrect, 888 = no-choice, 999 = immature trial)\n",
    "                            10: Sd presentation duration (Sd onset --> choice)\n",
    "                            11: Reaction time (Levers presentation --> choice)\n",
    "                            12: Lever choice pattern (0 = stay, 1 = shift, 999 = none)\n",
    "                            '''\n",
    "                            # df_Xとdf_Yの値を処理しやすい形式に計算する。列名は小文字にする。\n",
    "                            df_to_add = pd.DataFrame()\n",
    "                            # 1セッションあたりの時間 * 行番号 = このセッション開始からの経過時間\n",
    "                            df_to_add['trial_time_stamp'] = pd.Series((df_X[1] - 1) * ((end_datetime_of_this_session - start_datetime_of_this_session) / (index_of_last_row + 1)) + first_datetime_of_this_MSN_session)\n",
    "                            # このMSNの一番最初のセッションのStart Date + (1セッションあたりの時間 * 行番号) = このMSNの一番最初のセッションからの経過時間 \n",
    "                            df_to_add['trial_time_delta'] = pd.Series((df_X[1] - 1) * ((end_datetime_of_this_session - start_datetime_of_this_session) / (index_of_last_row + 1)) + (start_datetime_of_this_session - first_datetime_of_this_MSN_session)).dt.total_seconds()\n",
    "                            # subjectの名前\n",
    "                            df_to_add['subject'] = a_subject_key\n",
    "                            # msnの名前\n",
    "                            df_to_add['msn'] = MSN_of_this_session\n",
    "                            # 余分にnosepokeした時間 = 実際にnosepokeした時間 - 求められた時間\n",
    "                            df_to_add['extra_nosepoke_time(s)'] = pd.to_timedelta(df_X[5] - df_X[4], unit='S').dt.total_seconds()\n",
    "                            # nosepokeが成功したかどうか\n",
    "                            df_to_add['success_nosepoke'] = (df_X[5] >= df_X[4])\n",
    "                            # lever choiceが正しいかどうか\n",
    "                            df_to_add.loc[df_Y[3] <= 2, 'correct_leverchoice'] = (df_Y[3] <= 1) | ~(df_Y[3] == 2)\n",
    "                            # どのleverを選択したか\n",
    "                            df_to_add['leverchoice:_right=1_left=-1_other=0'] = (df_Y[3] == 1)*1 + (df_Y[3] == 0)*(-1) + (df_Y[3] > 1)*0\n",
    "\n",
    "                            # 以上の処理を行ったdataframeを結合し、行名列名を付ける\n",
    "                            df_of_this_session = pd.concat([df_X.iloc[:, 1:], df_Y.iloc[:, 1:], df_to_add], axis='columns')\n",
    "                            list_of_df_X_columns = ['order_of_trial', 'nosepoke_in_20min', 'nosepoke_latency', 'nosepoke_duration_required', 'nosepoke_duration_measured', 'empty']\n",
    "                            list_of_df_X_columns = [f'X{i+1}_{list_of_df_X_columns[i]}' for i in range(len(list_of_df_X_columns))]\n",
    "                            list_of_df_Y_columns = ['sd_presented', 'lever_pressed', 'outcome', 'sd_presentation_duration', 'reaction_time', 'lever_choice_pattern']\n",
    "                            list_of_df_Y_columns = [f'Y{i+1}_{list_of_df_Y_columns[i]}' for i in range(len(list_of_df_Y_columns))]\n",
    "                            list_of_df_to_add_columns = df_to_add.columns.to_list()\n",
    "                            MultiColmins_of_df_of_this_session = pd.MultiIndex.from_product([[f'{a_subject_key}'], list_of_df_X_columns + list_of_df_Y_columns + list_of_df_to_add_columns], names=['subject', 'data'])\n",
    "                            \n",
    "                            MultiIndex_of_df_of_this_session = pd.MultiIndex.from_product([[MSN_of_this_session], df_of_this_session.index.to_list()], names=['MSN', 'index'])\n",
    "                            ############################################################\n",
    "                            #print('check point 1')\n",
    "                            #if a_subject_key == 'I322':\n",
    "                            #    display(df_of_this_session)\n",
    "                            ############################################################\n",
    "                            df_of_this_session = pd.DataFrame(df_of_this_session.values, index=MultiIndex_of_df_of_this_session, columns=MultiColmins_of_df_of_this_session)\n",
    "# 処理終わり              \n",
    "############################################################\n",
    "                            # このdfをdf_of_this_subjectに結合する\n",
    "                            df_of_this_subject = pd.concat([df_of_this_subject, df_of_this_session], axis='index')\n",
    "                        # これで1つのsessionの処理が終わった\n",
    "\n",
    "                    # このsubjectに関するまとめの処理の開始\n",
    "                    if df_of_this_subject.empty:\n",
    "                        print(f'{a_subject_key}: This rat didn`t do the target MSN')\n",
    "############################################################\n",
    "# DOEの弁別訓練のための処理        \n",
    "                    else:\n",
    "                        #print('check point 0')\n",
    "                        #列名をMultiIndexにしてるので、行冥の指定には、DataFrame[level1の列名][level2の列名]という形で指定する\n",
    "                        df_of_this_subject_nosepoke_isnot_false = df_of_this_subject[df_of_this_subject[a_subject_key]['correct_leverchoice'].notna()]\n",
    "                        df_of_this_subject_nosepoke_isnot_false = df_of_this_subject_nosepoke_isnot_false.reset_index(drop=True)\n",
    "                        df_of_this_subject = df_of_this_subject.reset_index(drop=True)\n",
    "                        list_of_moving_average_columns = ['success_nosepoke_moving_average', 'correct_leverchoice_moving_average', 'leverchoice_bias_moving_average']\n",
    "                        MultiColumns_of_df_of_this_subject = pd.MultiIndex.from_product([[f'{a_type_of_experiment.lower()}_{a_subject_key}'], list_of_moving_average_columns], names=['subject', 'data'])\n",
    "\n",
    "                        # まずは、nosepokeが失敗したときを含む処理\n",
    "                        df_of_analyse_result_of_this_subject = pd.DataFrame()\n",
    "                        df_of_analyse_result_of_this_subject = pd.concat([df_of_analyse_result_of_this_subject, df_of_this_subject[a_subject_key]['success_nosepoke'].rolling(window_size, min_periods=1).mean()], axis='columns')                       \n",
    "                        df_of_analyse_result_of_this_subject = pd.concat([df_of_analyse_result_of_this_subject, df_of_this_subject[a_subject_key]['correct_leverchoice'].rolling(window_size, min_periods=1).mean()], axis='columns')\n",
    "                        df_of_analyse_result_of_this_subject = pd.concat([df_of_analyse_result_of_this_subject, df_of_this_subject[a_subject_key]['leverchoice:_right=1_left=-1_other=0'].rolling(window_size, min_periods=1).sum().abs() / df_of_this_subject[a_subject_key]['leverchoice:_right=1_left=-1_other=0'].abs().rolling(window_size, min_periods=1).sum()], axis='columns')\n",
    "                        df_of_analyse_result_of_this_subject.columns = MultiColumns_of_df_of_this_subject\n",
    "                        df_of_this_subject = pd.concat([df_of_this_subject, df_of_analyse_result_of_this_subject], axis='columns')\n",
    "                        df_of_this_subject.to_excel(f'{path_to_xlsx_folder}/{a_subject_key}.({a_type_of_experiment}_window{window_size}_withNULL).xlsx')\n",
    "                        #集計結果(nosepokeの失敗は不問)を、全体結果としてdfに追加\n",
    "                        \n",
    "                        ############################################################\n",
    "                        #if a_subject_key == 'SH013' or a_subject_key == 'SH012':\n",
    "                        #    print('check point 1')\n",
    "                        #    display(df_of_this_subject.columns)\n",
    "                        ############################################################\n",
    "                        df_of_this_type_of_experiment = pd.concat([df_of_this_type_of_experiment, df_of_analyse_result_of_this_subject], axis='columns')\n",
    "                        ############################################################\n",
    "                        #if a_subject_key == 'SH013' or a_subject_key == 'SH012':\n",
    "                        #    print('check point 2')\n",
    "                        #    #print(df_of_this_subject.columns)\n",
    "                        #    display(df_of_this_type_of_experiment.columns)\n",
    "                        ############################################################\n",
    "                        \n",
    "                        \n",
    "                        # 次に、nosepokeが失敗したときを除外した処理\n",
    "                        df_of_analyse_result_of_this_subject_nosepoke_isnot_false = pd.DataFrame()\n",
    "                        df_of_analyse_result_of_this_subject_nosepoke_isnot_false = pd.concat([df_of_analyse_result_of_this_subject_nosepoke_isnot_false, df_of_this_subject_nosepoke_isnot_false[a_subject_key]['success_nosepoke'].rolling(window_size, min_periods=1).mean()], axis='columns')\n",
    "                        df_of_analyse_result_of_this_subject_nosepoke_isnot_false = pd.concat([df_of_analyse_result_of_this_subject_nosepoke_isnot_false, df_of_this_subject_nosepoke_isnot_false[a_subject_key]['correct_leverchoice'].rolling(window_size, min_periods=1).mean()], axis='columns')\n",
    "                        df_of_analyse_result_of_this_subject_nosepoke_isnot_false = pd.concat([df_of_analyse_result_of_this_subject_nosepoke_isnot_false, df_of_this_subject_nosepoke_isnot_false[a_subject_key]['leverchoice:_right=1_left=-1_other=0'].rolling(window_size, min_periods=1).sum().abs() / df_of_this_subject_nosepoke_isnot_false[a_subject_key]['leverchoice:_right=1_left=-1_other=0'].abs().rolling(window_size, min_periods=1).sum()], axis='columns')\n",
    "                        df_of_analyse_result_of_this_subject_nosepoke_isnot_false.columns = MultiColumns_of_df_of_this_subject\n",
    "                        df_of_this_subject_nosepoke_isnot_false = pd.concat([df_of_this_subject_nosepoke_isnot_false, df_of_analyse_result_of_this_subject_nosepoke_isnot_false], axis='columns')\n",
    "                        df_of_this_subject_nosepoke_isnot_false.to_excel(f'{path_to_xlsx_folder_nosepoke_isnot_false}/{a_subject_key}.({a_type_of_experiment}_window{window_size}_withoutNULL).xlsx')\n",
    "                        #集計結果(nosepokeの失敗は除外)を、全体結果としてdfに追加\n",
    "                        df_of_this_type_of_experiment_nosepoke_isnot_false = pd.concat([df_of_this_type_of_experiment_nosepoke_isnot_false, df_of_analyse_result_of_this_subject_nosepoke_isnot_false], axis='columns')\n",
    "                        \n",
    "                        print(f'{a_subject_key}, WindowSize{window_size}: \\tDONE!! xlsx file has been exported!!')\n",
    "                    # これでひとつのsubjectの処理が終わった\n",
    "                # これでひとつのjsonの処理が終わった\n",
    "\n",
    "        #list of df to make statusをループの前に置いてしまったので、最後のpd.concatが反映されてなかった。\n",
    "        list_of_df_to_make_status = [df_of_this_type_of_experiment, df_of_this_type_of_experiment_nosepoke_isnot_false]\n",
    "        for i in range(len(list_of_df_to_make_status)):\n",
    "            a_df = list_of_df_to_make_status[i]\n",
    "            # swaplevelは、順序を変更したデータのコピーを返すので、元のデータは変更されないらしい。\n",
    "            a_df = a_df.swaplevel('subject', 'data', axis='columns')\n",
    "            df_to_add = pd.DataFrame()\n",
    "            for a_colmun in list_of_moving_average_columns:\n",
    "                df_of_status = pd.DataFrame()\n",
    "                df_of_status = pd.concat([df_of_status, a_df[a_colmun].mean(axis='columns')], axis='columns')\n",
    "                df_of_status = pd.concat([df_of_status, a_df[a_colmun].std(axis='columns')], axis='columns')\n",
    "                df_of_status = pd.concat([df_of_status, a_df[a_colmun].sem(axis='columns')], axis='columns') \n",
    "                MultiColmins_of_new_df = pd.MultiIndex.from_product([[a_colmun], [f'{a_type_of_experiment.lower()}_average', f'{a_type_of_experiment.lower()}_std', f'{a_type_of_experiment.lower()}_sem']], names=['data', 'subject'])\n",
    "                df_of_status.columns = MultiColmins_of_new_df\n",
    "                df_to_add = pd.concat([df_to_add, df_of_status], axis='columns')\n",
    "            df_to_add = df_to_add.swaplevel('subject', 'data', axis='columns')\n",
    "            list_of_df_to_make_status[i] = pd.concat([list_of_df_to_make_status[i], df_to_add], axis='columns')\n",
    "            list_of_df_to_make_status[i] = list_of_df_to_make_status[i].swaplevel('data','subject',  axis='columns')\n",
    "            list_of_df_to_make_status[i] = list_of_df_to_make_status[i].sort_index(axis='columns')\n",
    "\n",
    "            \n",
    "        #########################\n",
    "        # めちゃくちゃ混乱した。\n",
    "        # df_of_this_type_of_experimentと、list_of_df_to_make_status[0]は、同じオブジェクトだと思っていたが、違った。\n",
    "        # 違うオブジェクトらしい。 \n",
    "        # これが原因でdf_of_statusが反映されてなかった\n",
    "        # リストに格納した時点で、違うオブジェクトになっているので、元のオブジェクトには反映されない。\n",
    "\n",
    "        #print('check point 3')\n",
    "        #display(list_of_df_to_make_status[0].columns)\n",
    "        #display(df_of_this_type_of_experiment)\n",
    "        #########################\n",
    "# 処理終わり    \n",
    "############################################################\n",
    "        \n",
    "        #print('check point 1')\n",
    "        #display(list_of_df_to_make_status[0])\n",
    "        df_of_this_folder = pd.concat([df_of_this_folder, list_of_df_to_make_status[0]], axis='columns')\n",
    "        df_of_this_folder_nosepoke_isnot_false = pd.concat([df_of_this_folder_nosepoke_isnot_false, list_of_df_to_make_status[1]], axis='columns')\n",
    "        # これでひとつのtype_of_experimentの処理が終わった\n",
    "\n",
    "    # swaplevelは、順序を変更したデータのコピーを返すので、元のデータは変更されないらしい。\n",
    "    df_of_this_folder = df_of_this_folder.sort_index(axis='columns', level='data')\n",
    "    df_of_this_folder.to_excel(f'{path_to_xlsx_folder}/all_rats.(mod1_window{window_size}_withNULL).xlsx')\n",
    "    df_of_this_folder.iloc[df_of_this_folder.index % window_size == (window_size - 1), :].to_excel(f'{path_to_xlsx_folder}/all_rats.(mod{window_size}_window{window_size}_withNULL).xlsx')\n",
    "    \n",
    "    df_of_this_folder_nosepoke_isnot_false = df_of_this_folder_nosepoke_isnot_false.sort_index(axis='columns', level='data')\n",
    "    df_of_this_folder_nosepoke_isnot_false.to_excel(f'{path_to_xlsx_folder_nosepoke_isnot_false}/all_rats.(mod1_window{window_size}_withoutNULL).xlsx')\n",
    "    df_of_this_folder_nosepoke_isnot_false.iloc[df_of_this_folder_nosepoke_isnot_false.index % window_size == (window_size - 1), :].to_excel(f'{path_to_xlsx_folder_nosepoke_isnot_false}/all_rats.(mod{window_size}_window{window_size}_withoutNULL).xlsx')\n",
    "    # これでひとつのfolderの処理が終わった         \n",
    "\n",
    "# 出力されたxlsxファイルの中身が、おかしくないか調べる関数\n",
    "def check_the_number_of_trial_of_is_match_between_xlsx_and_memo(path_to_output_folder):\n",
    "    path_to_json_data_folder = f'{path_to_output_folder}/json_from_RawData'\n",
    "    path_of_all_data = sorted(glob.glob(path_to_json_data_folder + '/*[.json]')) \n",
    "    list_of_target = []\n",
    "    for a_type in dict_of_target.keys():\n",
    "        list_of_target.append([i.upper() for i in dict_of_target[a_type]])\n",
    "    df_of_this_folder = pd.DataFrame()\n",
    "    # ラットごとの、jsonファイルのパス\n",
    "    for path_of_a_data in path_of_all_data:\n",
    "        # ラット一匹分のjsonファイルを開く\n",
    "        with open(path_of_a_data) as json_open:\n",
    "            json_load = json.load(json_open)\n",
    "\n",
    "            # 一番上の階層のkeyから、\n",
    "            # ラットの名前を取得\n",
    "            for key_of_a_subject in json_load.keys():\n",
    "                if key_of_a_subject in list_of_target:\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                df_of_this_subject = pd.DataFrame()\n",
    "                multiindex_of_this_folder = pd.MultiIndex.from_product([[key_of_a_subject], ['Start_Date', 'MSN', 'key_of_a_session', 'the_number_of_trial', 'sum_of_MSN']], names=['Subject', 'DATA'])\n",
    "                first_MSN_sesssion_datetime = 0\n",
    "                last_MSN_session = False\n",
    "                sum_of_trial = 0\n",
    "\n",
    "                # ２番目の階層のkeyから\n",
    "                # セッションの名前（MSN_date）を取得\n",
    "                for key_of_a_session in json_load[key_of_a_subject].keys():\n",
    "                    json_of_this_session = json_load[key_of_a_subject][key_of_a_session]\n",
    "                    MSN_of_this_session = json_of_this_session[\"MSN\"]\n",
    "                    start_datetime = dt.strptime(json_of_this_session[\"Start Date\"] + json_of_this_session[\"Start Time\"], '%m/%d/%y%H:%M:%S')\n",
    "                \n",
    "                    the_number_of_trail = 0\n",
    "                    keys_of_data = json_of_this_session.keys()\n",
    "                    for a_key in keys_of_data:\n",
    "\n",
    "                        if isinstance(json_of_this_session[a_key], list):\n",
    "                            if len(json_of_this_session[a_key]) == 1:\n",
    "                                pass\n",
    "                            elif isinstance(json_of_this_session[a_key][1], list):\n",
    "                                a_df_of_this_session = pd.DataFrame(json_of_this_session[a_key]).astype(float)\n",
    "                                index_of_last_row = 0\n",
    "                                for index, row in a_df_of_this_session.iterrows():\n",
    "                                    if row[1:].sum() != 0:\n",
    "                                        index_of_last_row = index\n",
    "                                    elif row[1:].sum() == 0:\n",
    "                                        pass\n",
    "                                edited_df_of_this_session = a_df_of_this_session.iloc[:index_of_last_row+1, :]\n",
    "                                if the_number_of_trail <= len(edited_df_of_this_session):\n",
    "                                    the_number_of_trail = len(edited_df_of_this_session)\n",
    "                            \n",
    "                    if MSN_of_this_session == last_MSN_session:\n",
    "                        sum_of_trial += the_number_of_trail\n",
    "                    elif MSN_of_this_session != last_MSN_session:\n",
    "                        last_MSN_session = MSN_of_this_session\n",
    "                        sum_of_trial = the_number_of_trail\n",
    "\n",
    "                    start_datetime = str(start_datetime).replace(' ', '_').replace(':', '-')\n",
    "                    a_df_of_this_session = pd.DataFrame([[start_datetime, MSN_of_this_session, key_of_a_session, the_number_of_trail, sum_of_trial]], columns=multiindex_of_this_folder)\n",
    "                    df_of_this_subject = pd.concat([df_of_this_subject, a_df_of_this_session], axis=0)\n",
    "                    df_of_this_subject.reset_index(drop=True, inplace=True)\n",
    "                df_of_this_folder = pd.concat([df_of_this_folder, df_of_this_subject], axis=1)\n",
    "                print(f'DONE!: {key_of_a_subject}')\n",
    "    #display(df_of_this_folder)\n",
    "    df_of_this_folder.to_excel(f'{path_to_output_folder}/the_number_of_trial.xlsx')\n",
    "\n",
    "\n",
    "def make_graph_from_xlsx(path_to_output_folder):\n",
    "    path_to_graph_folder = path_to_output_folder + '/graph'\n",
    "    if not os.path.exists(path_to_graph_folder):\n",
    "        os.mkdir(path_to_graph_folder)\n",
    "    elif os.path.exists(path_to_graph_folder):\n",
    "        shutil.rmtree(path_to_graph_folder)\n",
    "        os.mkdir(path_to_graph_folder)\n",
    "    \n",
    "    list_of_path_to_xlsx_folder = sorted(glob.glob(path_to_output_folder + '/xlsx_from_json' + '*'))\n",
    "\n",
    "    #print('check point 1')\n",
    "    #print(list_of_path_to_xlsx_folder)\n",
    "\n",
    "    for a_path_to_xlsx in list_of_path_to_xlsx_folder:\n",
    "        # print(a_path)\n",
    "        list_of_path_to_all_rats_xlsx = sorted(glob.glob(a_path_to_xlsx + '/all_rats' + '*' + '.xlsx'))\n",
    "        variation_of_this_folder = a_path_to_xlsx.split('.')[1]\n",
    "        largest_mod_num = 0\n",
    "        a_path_to_all_rats_xlsx_largest_mod = ''\n",
    "\n",
    "        #modが最大のxlsxファイルを選択\n",
    "        for a_path_to_all_rats in list_of_path_to_all_rats_xlsx:\n",
    "            mod_num = re.findall('mod(\\d+)', a_path_to_all_rats)\n",
    "            if int(mod_num[0]) > largest_mod_num:\n",
    "                largest_mod_num = int(mod_num[0])\n",
    "                a_path_to_all_rats_xlsx_largest_mod = a_path_to_all_rats\n",
    "\n",
    "        # header=[0,1]で、1行目と2行目をヘッダーとして読み込む\n",
    "        df_of_all_rats = pd.read_excel(a_path_to_all_rats_xlsx_largest_mod, header=[0,1], index_col=[0])\n",
    "\n",
    "        # make figure\n",
    "        list_of_data = df_of_all_rats.columns.get_level_values('data').unique().to_list()\n",
    "        list_of_subject = df_of_all_rats.columns.get_level_values('subject').unique().to_list()\n",
    "        list_of_type_of_experiment = sorted(set([i.split('_')[0].upper() for i in df_of_all_rats.columns.get_level_values('subject').unique().tolist()]))\n",
    "        color_map = [plt.cm.tab20(i) for i in range(len(list_of_type_of_experiment) * 2)]\n",
    "        color_map_index = 0\n",
    "        fig, axis = plt.subplots(1, len(list_of_data), figsize=(30, 10), dpi=300)\n",
    "        for a_data_No in range(len(list_of_data)):\n",
    "            a_data = list_of_data[a_data_No]\n",
    "            for a_type_No in range(len(list_of_type_of_experiment)):\n",
    "                a_type = list_of_type_of_experiment[a_type_No].upper()\n",
    "                \n",
    "                list_of_this_type_subject = [i for i in list_of_subject if i.split('_')[0].upper() == a_type]\n",
    "                list_of_this_type_subject.sort()\n",
    "                for a_subject in list_of_this_type_subject:\n",
    "                    df_of_this_type_subject = df_of_all_rats[a_data][a_subject]\n",
    "                    if re.findall('average', a_subject):\n",
    "                        axis[a_data_No].plot(df_of_this_type_subject, color=color_map[color_map_index], label=a_subject, alpha=1.0)\n",
    "                    elif not re.findall('std', a_subject) and not re.findall('sem', a_subject):\n",
    "                        axis[a_data_No].plot(df_of_this_type_subject, color=color_map[color_map_index + 1], label=a_subject, alpha=0.2)\n",
    "                color_map_index += 2\n",
    "            color_map_index = 0\n",
    "\n",
    "        for a_data_No in range(len(list_of_data)):\n",
    "            a_data = list_of_data[a_data_No]\n",
    "            colmun = a_data.split('_moving_average')[0]\n",
    "            axis[a_data_No].set_title(f'{colmun}_{variation_of_this_folder}')\n",
    "            axis[a_data_No].legend()\n",
    "            axis[a_data_No].set_xlabel('traial')\n",
    "            axis[a_data_No].set_ylabel('percentage (%)')\n",
    "        # テキストをテキストとして出力する設定\n",
    "        plt.rc(\"svg\", fonttype=\"none\")\n",
    "        # SVGとして保存\n",
    "        fig.savefig(f'{path_to_graph_folder}/{variation_of_this_folder}.svg', format='svg')\n",
    "        # PNGとして保存\n",
    "        fig.savefig(f'{path_to_graph_folder}/{variation_of_this_folder}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [path_to_RawData_folder]:\n",
    "    make_json_from_RawData(i)\n",
    "    repaire_known_issue_of_json(i)\n",
    "    for j in window_size:\n",
    "        make_xlsx_about_DOE_from_json(i, dict_of_target, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_from_xlsx(path_to_RawData_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
